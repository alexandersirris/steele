# -*- coding: utf-8 -*-
"""
Created on Tue Dec  6 22:38:56 2022

@reviewer: steelandquill

product churn dummy script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJI-819jMX6oFOXsNIESAkXyL3jZTrzL
"""

#%% Section: Import libraries
# Import numpy and pandas package
import os
import time
from datetime import datetime
from pytz import timezone
import numpy as np
import pandas as pd

# Move 'UID' to first column in the two dataframes
# col_to_first(day_1, 'UID')
# col_to_first(day_2, 'UID')
# Running the codeblock here to check. Let's see how these DFs look. It worked.

# pd.set_option('max_columns', None)
# double_df.head()

#%% Section: Import libraries
#%% Section: Define Functions

# Sets timezone to EST
def timestamp():
  # define date format
  fmt = '%Y-%m-%d %H:%M:%S'
  # define eastern timezone
  eastern = timezone('US/Eastern')
  # localized datetime
  loc_dt = datetime.now(eastern)
  # 2015-12-31 19:21:00 
  return(loc_dt.strftime(fmt))

# Moves specific column to the beginning of the dataframe
def col_to_first(df, column):
    # define column 'column' to be moved to first position
    first_column = df.pop(column)

    # insert column using insert(position,column_name, first_column) function
    df.insert(0, column, first_column)
    print("Column", column, "was shifted to first position")

# Strips suffix from column where there is a join 
def strip_right(df, suffix):
    df.columns = df.columns.str.rstrip(suffix)
    return df

def drop_dup_col(df, suffix):
    # list comprehension of the cols that end with '_y'
    to_drop = [x for x in df if x.endswith(suffix)]
    df.drop(to_drop, axis=1, inplace=True)
    return df

# Removes the time column, and updates the column with a new timestamp
def update_time(df):
    df = df.drop(labels='date', axis='columns')
    df = df.assign(date = pd.to_datetime(timestamp()))
    return df

# Imports the csv that will be the top-level and applies your table settings. It's identical to the next function, but the name makes debugging easier
def import_main(main_csv):
    df = pd.read_csv(main_csv, index_col=None, usecols = ['UID', 'current_csm', 'shop_name', 'region', 'internal_url', 'revenue_365', 'is_product_enabled'])
    df.sort_values('UID',ascending = True, ignore_index= True)
    return df

# Imports the csv that will be the daily query and applies your table settings.
def import_daily(daily_csv):
    df = pd.read_csv(daily_csv, index_col=None, usecols = ['UID', 'current_csm', 'shop_name', 'region', 'internal_url', 'revenue_365', 'is_product_enabled'])
    df.sort_values('UID',ascending = True, ignore_index= True)
    return df

# Establish listener for TRUE/FALSE switch conditions
# Row-wise column filling function for the lambda row
def status_flipped(row):
    if row['change'] < 0:
        return "CHURN"
    elif row['change'] > 0:
        return "WIN"
    else:
        return np.nan

# Check for churn/wins
def check_changes(main_df, daily_df):
    matches = daily_df[daily_df['UID'].isin(main_df['UID'])]
    matches['old'] = main_df['is_product_enabled'][main_df['UID'].isin(daily_df['UID'])]
    matches['new'] = matches['is_product_enabled']
    matches['oldint'] = matches['old'] * 1.0
    matches['newint'] = matches['new'] * 1.0
    matches['change'] = matches['newint'] - matches['oldint']
    matches['transition'] = matches.apply(lambda row : status_flipped(row), axis=1) # lambda function row 
    output = matches[['UID', 'transition']]
    output.dropna(inplace=True)
    return output

# Brings in the main dataframe and the daily dataframe.
# Then, creates a dataframe of just the intersecting info and appends the new unique rows to the the main dataframe
# Returns the updated main dataframe and the inner-joined intersecting rows to be processed.
# Returns two dataframes, updated and exists. 
def daily_update(main, daily):
    inter = pd.merge(daily, main, on='UID', how='left', indicator=True)
    new = inter[inter._merge == 'left_only'].iloc[:,:-1]
    updated = pd.concat([main, new], ignore_index=True)
    exists = main.merge(daily, on='UID', how='inner', suffixes=("","_d2"), sort=True) # To be joined with changes
    return updated, exists

#%% Section: Declare all global variables

#%% Section: Main code body

# Read Historical CSV file 
# Convert Historical CSV to dataframe with select columns 
# h_csv_3 = "historical_dataset.csv" #this works because they're in the working folder
h_df3_spi = import_main("/Users/alexandersirris/repos/steele/new/dummy_data/historical_dataset.csv")
# h_df3_spi.sort_values('UID',ascending = True, ignore_index= True)
# h_df3_spi.head(30)

# Read New CSV File
# Convert New CSV File to dataframe with select columns
# n_csv_3 = "day_2.csv" # day 2 pull of the report
n_df3_spi = import_daily("/Users/alexandersirris/repos/steele/new/dummy_data/day_2.csv")
#n_df3_spi.sort_values('UID',ascending = True, ignore_index= True)

# A DF with that day's changes
changes = check_changes(h_df3_spi, n_df3_spi)

#%%
# Compare new CSV with updated historical CSV
# Inner join new table with historical data table will remove new records. 
df_all, double_df1 = daily_update(h_df3_spi, n_df3_spi)
double_df1 = double_df1.merge(changes, on='UID', how='inner', sort=True)
double_df1 = drop_dup_col(double_df1, "_d2") # Drops dup column 
double_df1 = double_df1.assign(date = pd.to_datetime(timestamp()))

# Create churn and win table. 
churn_table = double_df1.loc[double_df1['transition']== "CHURN"]
wins_table = double_df1.loc[double_df1['transition']== "WIN"]

# Cleaning the df_all dataframe to remove duplicate columns and strip suffix
df_all = drop_dup_col(df_all, '_y')
df_all = strip_right(df_all, '_x')
print(churn_table.head(20))

# double_df1 = double_df1.loc[(double_df1['is_product_enabled_d1'] == True) & (double_df1['is_product_enabled_d2'] == False), ['UID', 'current_csm_d2', 'shop_name_d2', 'region_d2', 'internal_url_d2', 'revenue_365_d2', 'is_product_enabled_d2']]
# double_df1.rename(columns={'current_csm_d2': 'current_csm', 'shop_name_d2': 'shop_name', 'region_d2':'region', 'internal_url_d2':'internal_url', 'revenue_365_d2': 'revenue_365', 'is_product_enabled_d2':'is_product_enabled'}, inplace=True)
# double_df1.sort_values('UID',ascending = True, ignore_index= True)


# Update historical csv file with new records and snopshot
# This will concatinate the historical dataframe with the new dataframe, then remove the last duplicate
# Keep = 'first' will refresh the data table with the newest data, while adding new records
#df_all = pd.concat([h_df3_spi,n_df3_spi],ignore_index=True).drop_duplicates(subset='UID', keep='last')
df_all = df_all.sort_values('UID',ascending = True, ignore_index= True)

# New Churned Accounts
spi_churned_new = double_df1
spi_churned_new = spi_churned_new.assign(date = pd.to_datetime(timestamp()))

# spi_churned_new = update_time(spi_chu
